{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from validator_collection import validators, checkers\n",
    "from urllib.request import urlopen\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from newspaper import Article\n",
    "import scrapy\n",
    "import requests\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_class_hrefs(url):\n",
    "    \"\"\"\n",
    "    Finds all urls pointed to by all links inside\n",
    "    'news' class div elements\n",
    "    \"\"\"\n",
    "    #https://stackoverflow.com/questions/36709165/beautifulsoup-object-of-type-response-has-no-len\n",
    "    response = requests.get(url)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    #https://stackoverflow.com/questions/32468801/how-to-get-href-from-an-a-tag-inside-a-div\n",
    "    links = [a['href'] for div in soup.find_all(\"div\", attrs={\"class\": \"blog-list-blog\"}) for a in div.find_all('a')]\n",
    "    return list(dict.fromkeys(links))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InvalidURL = list()\n",
    "def get_short_news(url):\n",
    "    if checkers.is_url(url):\n",
    "        response = requests.get(url)\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        short_news = soup.find(\"div\", attrs={\"class\": \"article-bnow-box\"}).h2.text\n",
    "    else:\n",
    "        short_news = \"Invalid URL given\"\n",
    "        InvalidURL.append(url)\n",
    "        \n",
    "    return short_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UnProcessedUrls = list()\n",
    "def get_news_details(url):\n",
    "\n",
    "    try:\n",
    "        toi_article = Article(url, language=\"en\")\n",
    "        toi_article.download()\n",
    "        toi_article.parse()\n",
    "        time.sleep(3)\n",
    "        toi_article.nlp()\n",
    "    \n",
    "        ArticleTitle = toi_article.title\n",
    "        ArticleText = toi_article.text\n",
    "        ArticleSummary = toi_article.summary\n",
    "        ArtPubDate = toi_article.publish_date\n",
    "        return ArticleTitle, ArticleText, ArticleSummary, ArtPubDate\n",
    "\n",
    "    except:\n",
    "        UnProcessedUrls.append(url)\n",
    "        return \"No Text\", \"No Text\", \"No Text\", \"No Text\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "News18_Articles = pd.DataFrame(columns = ['Date', 'Title', 'Text', 'Summary', 'Link'])\n",
    "invalid_url, ArtTitle, ArtTest, ArtSummary, ArtDate, processedURLs = ([] for i in range(6))\n",
    "for i in range(72,100):\n",
    "    print('Number of Pgs Completed = ', i)\n",
    "    #https://www.news18.com/politics/page-1/\n",
    "    pageUrl = 'https://www.news18.com/politics/page-'+'{}/'.format(i)\n",
    "    processedURLs.append(pageUrl)\n",
    "    ArtLinks = get_news_class_hrefs(pageUrl)\n",
    "    \n",
    "    #https://stackoverflow.com/questions/23013220/max-retries-exceeded-with-url-in-requests\n",
    "    for artLink in ArtLinks:\n",
    "        if checkers.is_url(artLink):\n",
    "            if not None:\n",
    "                Title, Text, Summary, Date = get_news_details(artLink)\n",
    "                #short_news = get_short_news(artLink)\n",
    "                temp_df = pd.DataFrame({'Date' : Date,\n",
    "                                        'Title' : Title,\n",
    "                                        'Text' : Text,\n",
    "                                        'Summary' : Summary,\n",
    "                                        'Link': artLink}, index=[0])\n",
    "                News18_Articles = News18_Articles.append(temp_df, ignore_index=True, sort=True)\n",
    "                ArtTitle.append(Title)\n",
    "                ArtTest.append(Text)\n",
    "                ArtSummary.append(Summary)\n",
    "                ArtDate.append(Date)\n",
    "                #Short_News.append(short_news)\n",
    "            \n",
    "        else:\n",
    "            invalid_url.append(artLink)\n",
    "    print('Total News Articles Scrapped =', len(ArtTitle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "News18_Articles.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(News18_Articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "News18_Articles.drop_duplicates(subset = 'Title', keep = 'first', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(News18_Articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "News18_Articles.to_csv(r'C:\\Users\\shrikanth\\Desktop\\University of Passau\\Text Mining Project\\Project Implementation\\CSV Files\\News18\\News18_Articles_II.csv', \n",
    "            index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
